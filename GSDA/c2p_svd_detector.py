"""
C2P + SVD Fusion Detector - SVD+LoRA Hybrid Version
èåˆæ–¹æ¡ˆ: q_proj/k_proj/out_projç”¨SVD, v_projç”¨LoRA, MLPå†»ç»“

â˜… æ ¸å¿ƒä¿®æ”¹ (æ ¹æ®EFFORTè®ºæ–‡åŸå§‹ä»£ç ):
1. q_proj, k_proj, out_proj â†’ SVD (å­¦ä¹ ä¼ªå½±æ¨¡å¼)
2. v_proj â†’ LoRA (å­¦ä¹ ç±»åˆ«æ¦‚å¿µ)
3. MLPå±‚ â†’ å®Œå…¨å†»ç»“ (ä¿æŒé¢„è®­ç»ƒçŸ¥è¯†)
4. Classifier â†’ ç®€å•Linearå±‚ (è®ºæ–‡é£æ ¼)
5. æŸå¤±å‡½æ•° â†’ BCEWithLogitsLoss
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer
from modeling_svd import inject_svd_into_clip_vision, SVDResidualLinear, collect_svd_losses
from typing import Optional, List

# â˜… å¯¼å…¥PEFTçš„LoRA
try:
    from peft import LoraConfig, get_peft_model, TaskType
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("Warning: peft library not found. LoRA will not be available.")
    print("Install with: pip install peft")


class C2P_SVD_LoRA_Detector(nn.Module):
    """
    â˜… C2P + SVD + LoRA æ··åˆæ£€æµ‹å™¨ (è®ºæ–‡é£æ ¼å®ç°)
    
    èåˆæ¶æ„:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  CLIP Vision Encoder (24å±‚)                                â”‚
    â”‚  â”œâ”€â”€ Transformer Layer                                      â”‚
    â”‚  â”‚   â”œâ”€â”€ Self-Attention                                     â”‚
    â”‚  â”‚   â”‚   â”œâ”€â”€ q_proj  â†’ [SVD r=1023]  â† EFFORT              â”‚
    â”‚  â”‚   â”‚   â”œâ”€â”€ k_proj  â†’ [SVD r=1023]  â† EFFORT              â”‚
    â”‚  â”‚   â”‚   â”œâ”€â”€ v_proj  â†’ [LoRA r=8]    â† C2P-CLIP            â”‚
    â”‚  â”‚   â”‚   â””â”€â”€ out_proj â†’ [SVD r=1023] â† EFFORT              â”‚
    â”‚  â”‚   â””â”€â”€ MLP                                                â”‚
    â”‚  â”‚       â”œâ”€â”€ fc1 â†’ [å†»ç»“]                                   â”‚
    â”‚  â”‚       â””â”€â”€ fc2 â†’ [å†»ç»“]                                   â”‚
    â”‚  â””â”€â”€ (é‡å¤24å±‚)                                             â”‚
    â”‚                                                             â”‚
    â”‚  â†’ pooler_output (1024ç»´)                                   â”‚
    â”‚  â†’ Classifier: Linear(1024, 1) [è®ºæ–‡é£æ ¼]                   â”‚
    â”‚  â†’ BCEWithLogitsLoss                                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    def __init__(self, 
                 clip_model_name='openai/clip-vit-large-patch14',
                 num_classes=1,  # â˜… è®ºæ–‡é»˜è®¤ä½¿ç”¨1ï¼ˆsigmoidï¼‰
                 svd_rank=1023,  # â˜… è®ºæ–‡é»˜è®¤n-r=1ï¼Œæ‰€ä»¥r=1024-1=1023
                 lora_rank=8,
                 lora_alpha=8.0,
                 lora_dropout=0.8,
                 use_text_guidance=True,
                 class_weights: Optional[torch.Tensor] = None,
                 init_gain=0.02):  # â˜… è®ºæ–‡çš„åˆå§‹åŒ–å¢ç›Š
        """
        Args:
            clip_model_name: CLIPæ¨¡å‹åç§°
            num_classes: åˆ†ç±»æ•°é‡ (1=sigmoid, 2=softmax)
            svd_rank: SVDä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ (è®ºæ–‡é»˜è®¤1023ï¼Œå³n-r=1)
            lora_rank: LoRAçš„ç§©
            lora_alpha: LoRAçš„ç¼©æ”¾å› å­
            lora_dropout: LoRAçš„dropoutç‡
            use_text_guidance: æ˜¯å¦å¯ç”¨C2Pæ–‡æœ¬å¼•å¯¼
            class_weights: ç±»åˆ«æƒé‡
            init_gain: åˆ†ç±»å™¨æƒé‡åˆå§‹åŒ–çš„æ ‡å‡†å·® (è®ºæ–‡é»˜è®¤0.02)
        """
        super().__init__()
        
        if not PEFT_AVAILABLE:
            raise ImportError("peft library is required for LoRA. Install with: pip install peft")
        
        self.use_text_guidance = use_text_guidance
        self.num_classes = num_classes
        self.svd_rank = svd_rank
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.init_gain = init_gain
        self.class_weights = class_weights
        
        print("\n" + "="*70)
        print("Initializing C2P + SVD + LoRA Fusion Detector (Paper Style)")
        print(f"  SVD (q/k/out_proj): rank={svd_rank}, residual_dim={1024-svd_rank}")
        print(f"  LoRA (v_proj): rank={lora_rank}, Î±={lora_alpha}, dropout={lora_dropout}")
        print(f"  MLP: Frozen")
        print(f"  Num Classes: {num_classes} ({'BCE+sigmoid' if num_classes == 1 else 'CE+softmax'})")
        print(f"  Classifier: Linear(1024, {num_classes}) [Paper Style]")
        print(f"  Text Guidance: {'Enabled' if use_text_guidance else 'Disabled'}")
        print("="*70)
        
        # ============ 1. åŠ è½½CLIPæ¨¡å‹ ============
        print(f"\nLoading CLIP model: {clip_model_name}")
        self.clip = CLIPModel.from_pretrained(clip_model_name)
        self.processor = CLIPProcessor.from_pretrained(clip_model_name)
        self.tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)
        
        # ============ 2. å†»ç»“æ‰€æœ‰CLIPå‚æ•° ============
        for param in self.clip.parameters():
            param.requires_grad = False
        
        # ============ 3. æ³¨å…¥SVDåˆ°q_proj, k_proj, out_proj ============
        print("\nâ˜… Step 1: Injecting SVD into q_proj, k_proj, out_proj...")
        self.clip.vision_model = inject_svd_into_clip_vision(
            self.clip.vision_model,
            r=svd_rank,
            svd_target_modules=['q_proj', 'k_proj', 'out_proj'],
            include_mlp=False
        )
        
        # â˜…â˜…â˜… å…³é”®ä¿®å¤ï¼šæ˜¾å¼å¯ç”¨SVDå‚æ•°è®­ç»ƒ â˜…â˜…â˜…
        self._enable_svd_training()
        
        # ============ 4. å¯¹v_projåº”ç”¨LoRA ============
        print("\nâ˜… Step 2: Applying LoRA to v_proj...")
        lora_config = LoraConfig(
            r=lora_rank,
            lora_alpha=lora_alpha,
            target_modules=["v_proj"],
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.FEATURE_EXTRACTION
        )
        
        self.clip.vision_model = get_peft_model(self.clip.vision_model, lora_config)
        print(f"  âœ“ LoRA applied to v_proj layers")
        
        # â˜…â˜…â˜… å†æ¬¡ç¡®ä¿SVDå‚æ•°å¯è®­ç»ƒï¼ˆPEFTå¯èƒ½ä¼šå½±å“ï¼‰ â˜…â˜…â˜…
        self._enable_svd_training()
        
        # ============ 5. è·å–ç‰¹å¾ç»´åº¦ ============
        # â˜… è®ºæ–‡ä½¿ç”¨ pooler_output (1024ç»´)ï¼Œä¸ä½¿ç”¨ visual_projection
        self.vision_hidden_size = 1024  # CLIP-ViT-Lçš„hidden size
        
        print(f"\n  Vision hidden size: {self.vision_hidden_size}")
        print(f"  Classifier input: pooler_output ({self.vision_hidden_size}D)")
        
        # ============ 6. åˆ†ç±»å¤´ (è®ºæ–‡é£æ ¼ï¼šç®€å•Linear) ============
        # â˜…â˜…â˜… å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨è®ºæ–‡çš„ç®€å•åˆ†ç±»å™¨ â˜…â˜…â˜…
        self.fc = nn.Linear(self.vision_hidden_size, num_classes)
        
        # â˜… è®ºæ–‡é£æ ¼çš„æƒé‡åˆå§‹åŒ–
        nn.init.normal_(self.fc.weight.data, mean=0.0, std=init_gain)
        if self.fc.bias is not None:
            nn.init.zeros_(self.fc.bias.data)
        
        print(f"  Classifier: Linear({self.vision_hidden_size}, {num_classes})")
        print(f"  Classifier params: {self.vision_hidden_size * num_classes + num_classes:,}")
        print(f"  Weight init: normal(0, {init_gain})")
        
        # ============ 7. æŸå¤±å‡½æ•° ============
        # â˜…â˜…â˜… å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨è®ºæ–‡çš„BCEæŸå¤± â˜…â˜…â˜…
        if num_classes == 1:
            # â˜… ä¿®å¤ï¼šæ­£ç¡®å¤„ç†è®¾å¤‡
            if class_weights is not None:
                # æ–¹æ¡ˆAï¼šå¦‚æœ class_weights æ˜¯ tensor
                if isinstance(class_weights, torch.Tensor):
                    pos_weight = (class_weights[1] / class_weights[0]).unsqueeze(0)
                # æ–¹æ¡ˆBï¼šå¦‚æœæ˜¯åˆ—è¡¨
                else:
                    pos_weight = torch.tensor(
                        [class_weights[1] / class_weights[0]], 
                        dtype=torch.float32
                    )
                self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
                print(f"  Loss: BCEWithLogitsLoss(pos_weight={pos_weight.item():.2f})")
            else:
                self.criterion = nn.BCEWithLogitsLoss()
                print(f"  Loss: BCEWithLogitsLoss")
        else:
            # CrossEntropy for multi-class
            if class_weights is not None:
                if isinstance(class_weights, torch.Tensor):
                    self.criterion = nn.CrossEntropyLoss(weight=class_weights)
                else:
                    self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights))
            else:
                self.criterion = nn.CrossEntropyLoss()
            print(f"  Loss: CrossEntropyLoss")
        
        # ============ 8. C2PåŸå‹ ============
        self.register_buffer('prototype_real', None)
        self.register_buffer('prototype_fake', None)
        
        # ============ 9. æ¸©åº¦å‚æ•° ============
        self.temperature = 0.07
        
        self._print_trainable_params()
    
    def _enable_svd_training(self):
        """
        â˜…â˜…â˜… å®Œå…¨ä¿®å¤ç‰ˆï¼šå¯ç”¨SVDæ®‹å·®å‚æ•°çš„è®­ç»ƒ â˜…â˜…â˜…
        ä¿®å¤ï¼š
        1. å‚æ•°å‘½å sigma_residual â†’ S_residual
        2. ä½¿ç”¨ modules() é€’å½’éå†
        3. æ£€æŸ¥ PEFT çš„ base_model
        """
        svd_params_enabled = 0
        svd_layers_count = 0
        checked_modules = set()  # é¿å…é‡å¤è®¡æ•°
        
        def enable_module_params(module):
            """å¯ç”¨å•ä¸ªSVDResidualLinearå±‚çš„å‚æ•°"""
            nonlocal svd_params_enabled, svd_layers_count
            
            # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ¨¡å—
            if id(module) in checked_modules:
                return
            checked_modules.add(id(module))
            
            if isinstance(module, SVDResidualLinear):
                svd_layers_count += 1
                
                # â˜… ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°å S_residual
                for param_name in ['S_residual', 'U_residual', 'V_residual']:
                    if hasattr(module, param_name):
                        param = getattr(module, param_name)
                        if param is not None and isinstance(param, nn.Parameter):
                            if not param.requires_grad:
                                param.requires_grad = True
                            svd_params_enabled += param.numel()
        
        # â˜… ä¿®å¤ï¼šä½¿ç”¨ modules() é€’å½’éå†ï¼ˆä¸æ˜¯ named_modulesï¼‰
        for module in self.clip.vision_model.modules():
            enable_module_params(module)
        
        # â˜… ä¿®å¤ï¼šå¦‚æœæœ‰ PEFT åŒ…è£…ï¼Œä¹Ÿæ£€æŸ¥ base_model
        if hasattr(self.clip.vision_model, 'base_model'):
            print(f"  âš  Detected PEFT wrapper, enabling SVD in base_model...")
            for module in self.clip.vision_model.base_model.modules():
                enable_module_params(module)
        
        print(f"\n  âœ“ SVD Training Status:")
        print(f"    - SVD layers found: {svd_layers_count}")
        print(f"    - SVD params enabled: {svd_params_enabled:,}")
        
        if svd_params_enabled == 0:
            print(f"  âš ï¸  CRITICAL: No SVD parameters were enabled!")
            self._debug_svd_params()
            return False
        
        return True
    
    def _print_trainable_params(self):
        """â˜… æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        svd_params = 0
        lora_params = 0
        classifier_params = 0
        
        for name, param in self.named_parameters():
            if param.requires_grad:
                if any(x in name for x in ['S_residual', 'U_residual', 'V_residual']):
                    if 'lora' not in name.lower():
                        svd_params += param.numel()
                        continue
                
                if 'lora' in name.lower():
                    lora_params += param.numel()
                elif 'fc.' in name or 'classifier' in name:
                    classifier_params += param.numel()
        
        print(f"\n{'='*70}")
        print(f"Parameter Statistics:")
        print(f"  Total: {total:,}")
        print(f"  Trainable: {trainable:,} ({trainable/total*100:.4f}%)")
        print(f"  Frozen: {total - trainable:,}")
        print(f"\n  Breakdown:")
        print(f"    SVD (q/k/out_proj): {svd_params:,} ({svd_params/max(trainable,1)*100:.2f}% of trainable)")
        print(f"    LoRA (v_proj): {lora_params:,} ({lora_params/max(trainable,1)*100:.2f}% of trainable)")
        print(f"    Classifier (fc): {classifier_params:,} ({classifier_params/max(trainable,1)*100:.2f}% of trainable)")
        print(f"{'='*70}\n")
        
        if svd_params == 0:
            print("âš ï¸  WARNING: No SVD parameters detected as trainable!")
            self._debug_svd_params()
    
    def _debug_svd_params(self):
        """è°ƒè¯•ï¼šè¯¦ç»†æ£€æŸ¥SVDå‚æ•°çŠ¶æ€"""
        print("\n  ğŸ” SVD Parameter Debug:")
        
        found_svd = False
        
        # æ£€æŸ¥æ‰€æœ‰æ¨¡å—
        all_modules = list(self.clip.vision_model.modules())
        if hasattr(self.clip.vision_model, 'base_model'):
            all_modules.extend(self.clip.vision_model.base_model.modules())
        
        for module in all_modules:
            if isinstance(module, SVDResidualLinear):
                found_svd = True
                print(f"\n    Found SVDResidualLinear:")
                
                for param_name in ['S_residual', 'U_residual', 'V_residual']:
                    if hasattr(module, param_name):
                        param = getattr(module, param_name)
                        if param is not None:
                            print(f"      {param_name}: "
                                  f"shape={param.shape}, "
                                  f"requires_grad={param.requires_grad}, "
                                  f"is_param={isinstance(param, nn.Parameter)}")
                break  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ª
        
        if not found_svd:
            print("    âœ— No SVDResidualLinear modules found!")
            print("    Check if SVD injection succeeded.")
    
    def _get_vision_model_forward(self):
        """
        â˜…â˜…â˜… å…³é”®ä¿®å¤ï¼šè·å–æ­£ç¡®çš„vision model forwardæ–¹æ³• â˜…â˜…â˜…
        å¤„ç†PEFTåŒ…è£…å™¨çš„é—®é¢˜
        """
        vision_model = self.clip.vision_model
        
        # æ£€æŸ¥æ˜¯å¦è¢«PEFTåŒ…è£…
        if hasattr(vision_model, 'base_model'):
            # PEFTåŒ…è£…çš„æƒ…å†µ
            if hasattr(vision_model.base_model, 'model'):
                # PeftModel -> LoraModel -> åŸå§‹æ¨¡å‹
                return vision_model.base_model.model
            else:
                return vision_model.base_model
        else:
            return vision_model
    
    def encode_image(self, images, return_feature=False):
        """
        â˜…â˜…â˜… ä¿®å¤ç‰ˆï¼šå›¾åƒç¼–ç  â˜…â˜…â˜…
        ä¿®å¤ï¼šæ­£ç¡®å¤„ç†PEFTåŒ…è£…ï¼Œé¿å…input_idsé”™è¯¯
        """
        batch_size, _, height, width = images.shape
        
        # å¼ºåˆ¶resizeåˆ°224x224ï¼ˆCLIPæ ‡å‡†è¾“å…¥ï¼‰
        if height != 224 or width != 224:
            images = F.interpolate(
                images, 
                size=(224, 224),
                mode='bilinear',
                align_corners=False
            )
        
        # â˜…â˜…â˜… å…³é”®ä¿®å¤ï¼šç›´æ¥è°ƒç”¨åº•å±‚æ¨¡å‹ï¼Œé¿å…PEFTçš„forwardé—®é¢˜ â˜…â˜…â˜…
        vision_model = self.clip.vision_model
        
        # æ–¹æ³•1ï¼šå°è¯•ç›´æ¥è°ƒç”¨ï¼ˆé€‚ç”¨äºæŸäº›PEFTç‰ˆæœ¬ï¼‰
        try:
            # è·å–åº•å±‚çš„vision encoder
            if hasattr(vision_model, 'base_model'):
                # PEFTåŒ…è£…
                if hasattr(vision_model.base_model, 'model'):
                    # PeftModel.base_model.model = åŸå§‹CLIPVisionModel
                    actual_model = vision_model.base_model.model
                else:
                    actual_model = vision_model.base_model
            else:
                actual_model = vision_model
            
            # ç›´æ¥è°ƒç”¨forward
            vision_outputs = actual_model(
                pixel_values=images,
                output_hidden_states=True,
                return_dict=True
            )
            
        except Exception as e:
            # æ–¹æ³•2ï¼šæ‰‹åŠ¨æ‰§è¡Œforwardæ­¥éª¤
            print(f"  Warning: Direct call failed ({e}), using manual forward...")
            vision_outputs = self._manual_vision_forward(images)
        
        pooler_output = vision_outputs.pooler_output
        
        if return_feature:
            return pooler_output
        
        return pooler_output
    
    def _manual_vision_forward(self, pixel_values):
        """
        â˜… æ‰‹åŠ¨æ‰§è¡Œè§†è§‰æ¨¡å‹çš„forwardï¼ˆä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
        """
        vision_model = self.clip.vision_model
        
        # è·å–åµŒå…¥å±‚
        if hasattr(vision_model, 'base_model'):
            if hasattr(vision_model.base_model, 'model'):
                embeddings = vision_model.base_model.model.embeddings
                encoder = vision_model.base_model.model.encoder
                pre_layrnorm = vision_model.base_model.model.pre_layrnorm
                post_layernorm = vision_model.base_model.model.post_layernorm
            else:
                embeddings = vision_model.base_model.embeddings
                encoder = vision_model.base_model.encoder
                pre_layrnorm = vision_model.base_model.pre_layrnorm
                post_layernorm = vision_model.base_model.post_layernorm
        else:
            embeddings = vision_model.embeddings
            encoder = vision_model.encoder
            pre_layrnorm = vision_model.pre_layrnorm
            post_layernorm = vision_model.post_layernorm
        
        # Forward pass
        hidden_states = embeddings(pixel_values)
        hidden_states = pre_layrnorm(hidden_states)
        
        encoder_outputs = encoder(
            inputs_embeds=hidden_states,
            output_hidden_states=True,
            return_dict=True
        )
        
        last_hidden_state = encoder_outputs.last_hidden_state
        pooler_output = post_layernorm(last_hidden_state[:, 0, :])
        
        # åˆ›å»ºè¾“å‡ºå¯¹è±¡
        class VisionOutputs:
            pass
        
        outputs = VisionOutputs()
        outputs.last_hidden_state = last_hidden_state
        outputs.pooler_output = pooler_output
        outputs.hidden_states = encoder_outputs.hidden_states
        
        return outputs
    
    def encode_text(self, text_list):
        """æ–‡æœ¬ç¼–ç  (ç”¨äºC2P)"""
        device = next(self.parameters()).device
        
        valid_texts = []
        for text in text_list:
            if text is not None and isinstance(text, str) and len(text.strip()) > 0:
                valid_texts.append(text)
            else:
                valid_texts.append("An image")
        
        inputs = self.tokenizer(
            valid_texts, 
            padding=True, 
            truncation=True, 
            max_length=77,
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            text_outputs = self.clip.text_model(**inputs)
            text_embeds = text_outputs.pooler_output
            text_features = self.clip.text_projection(text_embeds)
        
        text_features = F.normalize(text_features, p=2, dim=-1)
        
        return text_features
    
    def forward(self, images, labels=None, captions=None, return_feature=False):
        """
        â˜… å‰å‘ä¼ æ’­ (è®ºæ–‡é£æ ¼)
        
        è®ºæ–‡ä»£ç :
            features = self.model.vision_model(x)['pooler_output']
            if return_feature:
                return features
            return self.fc(features)
        """
        # è·å–è§†è§‰ç‰¹å¾
        features = self.encode_image(images, return_feature=True)  # [B, 1024]
        
        # åˆ†ç±»
        logits = self.fc(features)  # [B, num_classes]
        
        # â˜… æ¨ç†æ¨¡å¼
        if labels is None:
            if return_feature:
                return {'logits': logits, 'features': features}
            return logits
        
        # â˜… è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—æŸå¤±
        losses = {}
        
        # åˆ†ç±»æŸå¤±
        if self.num_classes == 1:
            # â˜… è®ºæ–‡é£æ ¼ï¼šBCEæŸå¤±
            loss_cls = self.criterion(logits.squeeze(-1), labels.float())
        else:
            loss_cls = self.criterion(logits, labels)
        losses['cls'] = loss_cls
        
        # SVDçº¦æŸæŸå¤±
        loss_ortho, loss_keepsv = self._compute_svd_losses()
        losses['ortho'] = loss_ortho
        losses['keepsv'] = loss_keepsv
        
        # C2PæŸå¤± (å¯é€‰)
        if self.use_text_guidance:
            # å¯¹äºC2Pï¼Œéœ€è¦å½’ä¸€åŒ–çš„ç‰¹å¾
            normalized_features = F.normalize(features, p=2, dim=-1)
            
            loss_prototype = self._compute_prototype_loss(normalized_features, labels)
            losses['prototype'] = loss_prototype
            
            if captions is not None:
                # ä¸ºcaption lossï¼Œéœ€è¦æŠ•å½±åˆ°æ–‡æœ¬ç©ºé—´
                projected = self.clip.visual_projection(features)
                projected = F.normalize(projected, p=2, dim=-1)
                loss_caption = self._compute_caption_loss(projected, captions, labels)
                losses['caption'] = loss_caption
            else:
                losses['caption'] = torch.tensor(0.0, device=images.device)
        else:
            losses['prototype'] = torch.tensor(0.0, device=images.device)
            losses['caption'] = torch.tensor(0.0, device=images.device)
        
        losses['logits'] = logits
        if return_feature:
            losses['features'] = features
        
        return losses
    
    def compute_losses(self, images, labels, texts=None):
        """è®¡ç®—æ‰€æœ‰æŸå¤± (è®­ç»ƒæ¥å£)"""
        result = self.forward(images, labels=labels, captions=texts)
        
        return {
            'cls': result['cls'],
            'prototype': result['prototype'],
            'caption': result['caption'],
            'ortho': result['ortho'],
            'keepsv': result['keepsv']
        }
    
    def _compute_svd_losses(self):
        """
        â˜… ä¿®å¤ç‰ˆï¼šè®¡ç®—SVDçº¦æŸæŸå¤±
        ä¿®å¤ï¼šç¡®ä¿æ”¶é›†åˆ°æ‰€æœ‰SVDæ¨¡å—
        """
        device = next(self.parameters()).device
        total_ortho = torch.tensor(0.0, device=device)
        total_keepsv = torch.tensor(0.0, device=device)
        count = 0
        
        # æ”¶é›†æ‰€æœ‰ SVDResidualLinear æ¨¡å—ï¼ˆé¿å…é‡å¤ï¼‰
        svd_modules = []
        checked_ids = set()
        
        # ä» vision_model æ”¶é›†
        for module in self.clip.vision_model.modules():
            if isinstance(module, SVDResidualLinear):
                if id(module) not in checked_ids:
                    svd_modules.append(module)
                    checked_ids.add(id(module))
        
        # ä» base_model æ”¶é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self.clip.vision_model, 'base_model'):
            for module in self.clip.vision_model.base_model.modules():
                if isinstance(module, SVDResidualLinear):
                    if id(module) not in checked_ids:
                        svd_modules.append(module)
                        checked_ids.add(id(module))
        
        # è®¡ç®—æŸå¤±
        for module in svd_modules:
            try:
                ortho_loss = module.compute_orthogonal_loss_decomposed()
                keepsv_loss = module.compute_keepsv_loss()
                
                total_ortho = total_ortho + ortho_loss
                total_keepsv = total_keepsv + keepsv_loss
                count += 1
            except Exception as e:
                print(f"Warning: Failed to compute SVD loss: {e}")
                continue
        
        if count > 0:
            return total_ortho / count, total_keepsv / count
        else:
            return total_ortho, total_keepsv
    
    def _compute_prototype_loss(self, features, labels):
        """è®¡ç®—åŸå‹å¯¹æ¯”æŸå¤± (C2P)"""
        device = features.device
        
        real_mask = (labels == 0)
        fake_mask = (labels == 1)
        
        real_features = features[real_mask]
        fake_features = features[fake_mask]
        
        momentum = 0.9
        
        if real_features.shape[0] > 0:
            real_proto = real_features.mean(dim=0)
            if self.prototype_real is None:
                self.prototype_real = real_proto.detach()
            else:
                self.prototype_real = momentum * self.prototype_real + (1 - momentum) * real_proto.detach()
        
        if fake_features.shape[0] > 0:
            fake_proto = fake_features.mean(dim=0)
            if self.prototype_fake is None:
                self.prototype_fake = fake_proto.detach()
            else:
                self.prototype_fake = momentum * self.prototype_fake + (1 - momentum) * fake_proto.detach()
        
        if self.prototype_real is None or self.prototype_fake is None:
            return torch.tensor(0.0, device=device)
        
        loss = torch.tensor(0.0, device=device)
        
        if real_features.shape[0] > 0:
            pos_sim = F.cosine_similarity(real_features, self.prototype_real.unsqueeze(0), dim=-1)
            neg_sim = F.cosine_similarity(real_features, self.prototype_fake.unsqueeze(0), dim=-1)
            loss = loss + (-torch.log(torch.exp(pos_sim / self.temperature) / 
                          (torch.exp(pos_sim / self.temperature) + torch.exp(neg_sim / self.temperature)))).mean()
        
        if fake_features.shape[0] > 0:
            pos_sim = F.cosine_similarity(fake_features, self.prototype_fake.unsqueeze(0), dim=-1)
            neg_sim = F.cosine_similarity(fake_features, self.prototype_real.unsqueeze(0), dim=-1)
            loss = loss + (-torch.log(torch.exp(pos_sim / self.temperature) / 
                          (torch.exp(pos_sim / self.temperature) + torch.exp(neg_sim / self.temperature)))).mean()
        
        return loss
    
    def _compute_caption_loss(self, image_features, captions, labels):
        """è®¡ç®—Captionå¯¹æ¯”æŸå¤± (C2P)"""
        batch_size = image_features.shape[0]
        device = image_features.device
        
        if batch_size < 2:
            return torch.tensor(0.0, device=device)
        
        text_features = self.encode_text(captions)
        
        sim_matrix = torch.mm(image_features, text_features.t()) / self.temperature
        pos_mask = torch.eye(batch_size, device=device).bool()
        
        loss = 0.0
        for i in range(batch_size):
            pos_sim = sim_matrix[i, i]
            neg_mask = ~pos_mask[i]
            neg_sims = sim_matrix[i, neg_mask]
            
            if neg_sims.numel() == 0:
                continue
            
            logits = torch.cat([pos_sim.unsqueeze(0), neg_sims])
            loss = loss + F.cross_entropy(logits.unsqueeze(0), torch.zeros(1, dtype=torch.long, device=device))
        
        return loss / batch_size
    
    def predict(self, images):
        """
        â˜… æ¨ç†æ¥å£ (è®ºæ–‡é£æ ¼)
        
        è¿”å›:
            å¦‚æœ num_classes=1: è¿”å›æ¦‚ç‡ (sigmoid)
            å¦‚æœ num_classes=2: è¿”å›ç±»åˆ«æ¦‚ç‡ (softmax)
        """
        self.eval()
        with torch.no_grad():
            logits = self.forward(images)
            
            if self.num_classes == 1:
                # BCE: sigmoidå¾—åˆ°fakeæ¦‚ç‡
                probs = torch.sigmoid(logits.squeeze(-1))
                preds = (probs > 0.5).long()
                return preds, probs
            else:
                # CE: softmaxå¾—åˆ°ç±»åˆ«æ¦‚ç‡
                probs = F.softmax(logits, dim=-1)
                preds = torch.argmax(probs, dim=-1)
                return preds, probs[:, 1]  # è¿”å›fakeçš„æ¦‚ç‡
    
    def get_fusion_status(self):
        """è·å–èåˆæ¶æ„çŠ¶æ€"""
        status = {
            'svd_layers': [],
            'lora_layers': [],
            'frozen_layers': []
        }
        
        # æ”¶é›†SVDå±‚
        checked_ids = set()
        for module in self.clip.vision_model.modules():
            if isinstance(module, SVDResidualLinear):
                if id(module) not in checked_ids:
                    checked_ids.add(id(module))
                    params = module.get_trainable_params()
                    status['svd_layers'].append({
                        'trainable_params': params['total'],
                        'trainable_ratio': params['ratio']
                    })
        
        if hasattr(self.clip.vision_model, 'base_model'):
            for module in self.clip.vision_model.base_model.modules():
                if isinstance(module, SVDResidualLinear):
                    if id(module) not in checked_ids:
                        checked_ids.add(id(module))
                        params = module.get_trainable_params()
                        status['svd_layers'].append({
                            'trainable_params': params['total'],
                            'trainable_ratio': params['ratio']
                        })
        
        # æ”¶é›†LoRAå±‚
        for name, param in self.clip.vision_model.named_parameters():
            if 'lora' in name.lower() and param.requires_grad:
                status['lora_layers'].append({
                    'name': name,
                    'shape': list(param.shape),
                    'params': param.numel()
                })
        
        return status
    
    def save_pretrained(self, save_path):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´çŠ¶æ€
        state_dict = {
            'model_state_dict': self.state_dict(),
            'config': {
                'num_classes': self.num_classes,
                'svd_rank': self.svd_rank,
                'lora_rank': self.lora_rank,
                'lora_alpha': self.lora_alpha,
                'use_text_guidance': self.use_text_guidance,
                'vision_hidden_size': self.vision_hidden_size,
            }
        }
        torch.save(state_dict, os.path.join(save_path, 'model.pt'))
        print(f"Model saved to {save_path}")
    
    @classmethod
    def load_pretrained(cls, load_path, clip_model_name='openai/clip-vit-large-patch14', device='cuda'):
        """åŠ è½½æ¨¡å‹"""
        import os
        state_dict = torch.load(os.path.join(load_path, 'model.pt'), map_location=device)
        config = state_dict['config']
        
        model = cls(
            clip_model_name=clip_model_name,
            num_classes=config['num_classes'],
            svd_rank=config['svd_rank'],
            lora_rank=config['lora_rank'],
            lora_alpha=config['lora_alpha'],
            use_text_guidance=config['use_text_guidance'],
        )
        
        model.load_state_dict(state_dict['model_state_dict'])
        model.to(device)
        print(f"Model loaded from {load_path}")
        return model


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================

def get_trainable_params(model):
    """è·å–å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    svd_params = 0
    lora_params = 0
    classifier_params = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if any(x in name for x in ['S_residual', 'U_residual', 'V_residual']):
                if 'lora' not in name.lower():
                    svd_params += param.numel()
                    continue
            
            if 'lora' in name.lower():
                lora_params += param.numel()
            elif 'fc.' in name or 'classifier' in name:
                classifier_params += param.numel()
    
    return {
        'total': total_params,
        'trainable': trainable_params,
        'frozen': frozen_params,
        'trainable_ratio': trainable_params / total_params * 100 if total_params > 0 else 0,
        'svd_params': svd_params,
        'lora_params': lora_params,
        'classifier_params': classifier_params
    }


# å‘åå…¼å®¹åˆ«å
C2P_SVD_Detector = C2P_SVD_LoRA_Detector


# ============================================================
# æµ‹è¯•ä»£ç 
# ============================================================

def test_fusion_detector():
    """æµ‹è¯•èåˆæ£€æµ‹å™¨"""
    print("\n" + "="*70)
    print("Testing C2P + SVD + LoRA Fusion Detector (Paper Style)")
    print("="*70)
    
    if not PEFT_AVAILABLE:
        print("\nâœ— peft library not found. Please install: pip install peft")
        return
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nDevice: {device}")
    
    # â˜… ä½¿ç”¨è®ºæ–‡é»˜è®¤å‚æ•°
    detector = C2P_SVD_LoRA_Detector(
        clip_model_name='openai/clip-vit-large-patch14',
        num_classes=1,      # â˜… è®ºæ–‡é£æ ¼ï¼šsigmoid
        svd_rank=1023,      # â˜… è®ºæ–‡é»˜è®¤ï¼šn-r=1
        lora_rank=8,
        lora_alpha=8.0,
        lora_dropout=0.8,
        use_text_guidance=True,
        init_gain=0.02      # â˜… è®ºæ–‡é»˜è®¤åˆå§‹åŒ–
    ).to(device)
    
    print("\n1. Testing trainable params...")
    params_info = get_trainable_params(detector)
    print(f"   Total: {params_info['total']:,}")
    print(f"   Trainable: {params_info['trainable']:,} ({params_info['trainable_ratio']:.4f}%)")
    print(f"   SVD params: {params_info['svd_params']:,}")
    print(f"   LoRA params: {params_info['lora_params']:,}")
    print(f"   Classifier params: {params_info['classifier_params']:,}")
    
    # â˜… éªŒè¯SVDå‚æ•°
    if params_info['svd_params'] > 0:
        print("   âœ“ SVD parameters are TRAINABLE!")
    else:
        print("   âœ— WARNING: SVD parameters are NOT trainable!")
    
    # â˜… éªŒè¯åˆ†ç±»å™¨å‚æ•° (è®ºæ–‡é£æ ¼åº”è¯¥æ˜¯ 1024*1+1 = 1025)
    expected_classifier_params = 1024 * 1 + 1  # Linear(1024, 1)
    print(f"   Expected classifier params: {expected_classifier_params}")
    
    print("\n2. Testing inference...")
    dummy_images = torch.randn(4, 3, 224, 224).to(device)
    with torch.no_grad():
        logits = detector(dummy_images)
    print(f"   Logits shape: {logits.shape}")  # åº”è¯¥æ˜¯ [4, 1]
    print(f"   Logits: {logits.squeeze().tolist()}")
    
    # â˜… æµ‹è¯•predictå‡½æ•°
    preds, probs = detector.predict(dummy_images)
    print(f"   Predictions: {preds.tolist()}")
    print(f"   Probabilities: {[f'{p:.4f}' for p in probs.tolist()]}")
    print("   âœ“ Inference works!")
    
    print("\n3. Testing training...")
    detector.train()
    dummy_labels = torch.randint(0, 2, (4,)).to(device)
    dummy_captions = ["Real face photo", "Fake deepfake face", "Natural photo", "AI generated"]
    
    losses = detector.compute_losses(dummy_images, dummy_labels, dummy_captions)
    print(f"   Losses: {list(losses.keys())}")
    print(f"   cls loss: {losses['cls'].item():.4f}")
    print(f"   ortho loss: {losses['ortho'].item():.6f}")
    print(f"   keepsv loss: {losses['keepsv'].item():.6f}")
    print(f"   prototype loss: {losses['prototype'].item():.4f}")
    print(f"   caption loss: {losses['caption'].item():.4f}")
    print("   âœ“ Training losses computed!")
    
    print("\n4. Testing gradient flow...")
    optimizer = torch.optim.AdamW(
        [p for p in detector.parameters() if p.requires_grad],
        lr=2e-4  # â˜… è®ºæ–‡ä½¿ç”¨çš„å­¦ä¹ ç‡
    )
    
    # å‰å‘ä¼ æ’­
    losses = detector.compute_losses(dummy_images, dummy_labels, dummy_captions)
    
    # â˜… è®ºæ–‡çš„æŸå¤±ç»„åˆ
    lambda1 = 0.1  # orthoæƒé‡
    lambda2 = 0.1  # keepsvæƒé‡
    total_loss = losses['cls'] + lambda1 * losses['ortho'] + lambda2 * losses['keepsv']
    
    if detector.use_text_guidance:
        total_loss = total_loss + 0.1 * losses['prototype'] + 0.1 * losses['caption']
    
    print(f"   Total loss: {total_loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    total_loss.backward()
    
    # æ£€æŸ¥å„ç»„ä»¶çš„æ¢¯åº¦
    print("\n   Gradient check:")
    
    # SVDæ¢¯åº¦
    svd_has_grad = False
    for name, param in detector.named_parameters():
        if 'S_residual' in name and param.grad is not None:
            if param.grad.abs().sum() > 0:
                svd_has_grad = True
                print(f"   âœ“ SVD ({name}): grad_mean={param.grad.abs().mean():.6f}")
                break
    if not svd_has_grad:
        print("   âœ— SVD: No gradient!")
    
    # LoRAæ¢¯åº¦
    lora_has_grad = False
    for name, param in detector.named_parameters():
        if 'lora' in name.lower() and param.grad is not None:
            if param.grad.abs().sum() > 0:
                lora_has_grad = True
                print(f"   âœ“ LoRA ({name}): grad_mean={param.grad.abs().mean():.6f}")
                break
    if not lora_has_grad:
        print("   âœ— LoRA: No gradient!")
    
    # åˆ†ç±»å™¨æ¢¯åº¦
    if detector.fc.weight.grad is not None:
        print(f"   âœ“ Classifier (fc.weight): grad_mean={detector.fc.weight.grad.abs().mean():.6f}")
    else:
        print("   âœ— Classifier: No gradient!")
    
    # ä¼˜åŒ–å™¨æ­¥è¿›
    optimizer.step()
    print("   âœ“ Optimizer step completed!")
    
    print("\n5. Testing multi-scale inference...")
    # æµ‹è¯•ä¸åŒå°ºå¯¸è¾“å…¥
    for size in [192, 224, 256, 384]:
        dummy_img = torch.randn(2, 3, size, size).to(device)
        with torch.no_grad():
            logits = detector(dummy_img)
        print(f"   Input {size}x{size} â†’ Output shape: {logits.shape} âœ“")
    
    print("\n6. Model architecture summary...")
    status = detector.get_fusion_status()
    print(f"   SVD layers: {len(status['svd_layers'])}")
    print(f"   LoRA layers: {len(status['lora_layers'])}")
    
    print("\n" + "="*70)
    print("All tests passed! âœ“")
    print("="*70)
    
    # â˜… æ‰“å°è®ºæ–‡é£æ ¼çš„å‚æ•°ç»Ÿè®¡
    print("\nğŸ“Š Paper-style Parameter Summary:")
    print(f"   Trainable params: {params_info['trainable']:,} â‰ˆ {params_info['trainable']/1e6:.2f}M")
    print(f"   (è®ºæ–‡æŠ¥å‘Š: 0.19M)")
    print()


def compare_with_paper():
    """ä¸è®ºæ–‡å‚æ•°å¯¹æ¯”"""
    print("\n" + "="*70)
    print("Comparing with EFFORT Paper")
    print("="*70)
    
    # è®ºæ–‡å‚æ•°
    paper_params = {
        'total_trainable': 190000,  # 0.19M
        'svd_rank': 1023,           # n-r=1
        'classifier': 1025,         # Linear(1024, 1)
    }
    
    print("\nè®ºæ–‡å®ç°:")
    print(f"  - SVD rank (r): {paper_params['svd_rank']} (residual dim = 1)")
    print(f"  - Classifier: Linear(1024, 1) = {paper_params['classifier']} params")
    print(f"  - Total trainable: ~{paper_params['total_trainable']:,} (0.19M)")
    
    print("\nä½ çš„å®ç° (ä¿®æ”¹å):")
    print(f"  - SVD rank (r): 1023 (residual dim = 1)")
    print(f"  - LoRA (v_proj): rank=8")
    print(f"  - Classifier: Linear(1024, 1) = 1,025 params")
    
    # è®¡ç®—é¢„æœŸå‚æ•°
    # SVD: 24å±‚ Ã— 3æŠ•å½±(q,k,out) Ã— (U_res + sigma_res + V_res)
    #    = 24 Ã— 3 Ã— (1024Ã—1 + 1 + 1Ã—1024) = 24 Ã— 3 Ã— 2049 = 147,528
    # LoRA: 24å±‚ Ã— 1æŠ•å½±(v) Ã— (A + B)
    #    = 24 Ã— 1 Ã— (1024Ã—8 + 8Ã—1024) = 24 Ã— 16384 = 393,216
    # Classifier: 1024 + 1 = 1,025
    
    svd_params = 24 * 3 * (1024 * 1 + 1 + 1 * 1024)
    lora_params = 24 * 1 * (1024 * 8 + 8 * 1024)
    classifier_params = 1024 * 1 + 1
    total = svd_params + lora_params + classifier_params
    
    print(f"\n  é¢„æœŸå‚æ•°è®¡ç®—:")
    print(f"    SVD (24å±‚Ã—3æŠ•å½±): {svd_params:,}")
    print(f"    LoRA (24å±‚Ã—1æŠ•å½±): {lora_params:,}")
    print(f"    Classifier: {classifier_params:,}")
    print(f"    Total: {total:,} ({total/1e6:.2f}M)")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    test_fusion_detector()
    compare_with_paper()

